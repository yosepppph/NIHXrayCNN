{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# NIH Chest X-ray Dataset"},{"metadata":{"_uuid":"de7893e221e7972c3719d0ca5f1c56ce366ba6cf"},"cell_type":"markdown","source":"<h1>1. Business Problem </h1>"},{"metadata":{"_uuid":"e53847a52e057f47dbad09fc0f998934ce0925d2"},"cell_type":"markdown","source":"<h2> 1.1 Description </h2>"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"## National Institutes of Health Chest X-Ray Dataset\n\nChest X-ray exams are one of the most frequent and cost-effective medical imaging examinations available. However, clinical diagnosis of a chest X-ray can be challenging and sometimes more difficult than diagnosis via chest CT imaging. The lack of large publicly available datasets with annotations means it is still very difficult, if not impossible, to achieve clinically relevant computer-aided detection and diagnosis (CAD) in real world medical sites with chest X-rays. One major hurdle in creating large X-ray image datasets is the lack resources for labeling so many images. Prior to the release of this dataset, Openi was the largest publicly available source of chest X-ray images with 4,143 images available.\n\nThis NIH Chest X-ray Dataset is comprised of 112,120 X-ray images with disease labels from 30,805 unique patients. To create these labels, the authors used Natural Language Processing to text-mine disease classifications from the associated radiological reports. The labels are expected to be >90% accurate and suitable for weakly-supervised learning. The original radiology reports are not publicly available but you can find more details on the labeling process in this Open Access paper: \"ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases.\" (Wang et al.)"},{"metadata":{"_uuid":"0f5c42cfaa6c5981669da854521144c1c10a6847"},"cell_type":"markdown","source":"### Data limitations:\n- 1. The image labels are NLP extracted so there could be some erroneous labels but the NLP labeling accuracy is estimated to be >90%.\n - 2. Very limited numbers of disease region bounding boxes (See BBox_list_2017.csv)\n - .Chest x-ray radiology reports are not anticipated to be publicly shared. Parties who use this public dataset are encouraged to share their “updated” image labels and/or new bounding boxes in their own studied later, maybe through manual annotation\n"},{"metadata":{"_uuid":"39d5ad1438dab73c79ecdd87f0d2532238f1978f"},"cell_type":"markdown","source":"<h2> 1.2 Source / useful links </h2>"},{"metadata":{"_uuid":"891d847d636d4d0a8299fa0e147be060d4da2ae1"},"cell_type":"markdown","source":"Data Source : https://www.kaggle.com/nih-chest-xrays/data/home <br>\nResearch paper : http://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf <br>\nResearch paper : https://lukeoakdenrayner.wordpress.com/2017/12/18/the-chestxray14-dataset-problems/ <br>\nResearch paper : https://arxiv.org/pdf/1711.05225.pdf <br>\nBlog : https://lukeoakdenrayner.wordpress.com/2018/01/24/chexnet-an-in-depth-review/"},{"metadata":{"_uuid":"ab43904a00138a5b8a9240da35f0103c8035cf3a"},"cell_type":"markdown","source":"<h2> 1.3 Real World / Business Objectives and Constraints </h2>"},{"metadata":{"_uuid":"c2f9e059ebcc79af68d03b91ff6c21d97731b566"},"cell_type":"markdown","source":"- 1. No strict latency constraints."},{"metadata":{"_uuid":"4e7f5fa52cc514c675c0abbab107edd128e19a4f"},"cell_type":"markdown","source":"<h2> 2.1 Data </h2>"},{"metadata":{"_uuid":"365997ab284ee358c0ea5822b737775ab004be6d"},"cell_type":"markdown","source":"### File contents\nImage format: 112,120 total images with size 1024 x 1024"},{"metadata":{"_uuid":"bc39a6322efc0cda30d1ff580ef546befd08ec16"},"cell_type":"markdown","source":"**BBox_list_2017.csv: **Bounding box coordinates. Note: Start at x,y, extend horizontally w pixels, and vertically h pixels\n\n- Image Index: File name\n- Finding Label: Disease type (Class label)\n- Bbox x\n- Bbox y\n- Bbox w\n- Bbox h"},{"metadata":{"_uuid":"aded9879a28af4fbbd7ccc6611af83123fc93353"},"cell_type":"markdown","source":"**Data_entry_2017.csv:** Class labels and patient data for the entire dataset\n- Image Index: File name\n- Finding Labels: Disease type (Class label)\n- Follow-up #\n- Patient ID\n- Patient Age\n- Patient Gender\n- View Position: X-ray orientation\n- OriginalImageWidth\n- OriginalImageHeight\n- OriginalImagePixelSpacing_x\n- OriginalImagePixelSpacing_y"},{"metadata":{"_uuid":"598d69a497e04c21edbb334d357fa1c172e3d03c"},"cell_type":"markdown","source":"**Class descriptions**\n\nThere are 15 classes (14 diseases, and one for \"No findings\"). Images can be classified as \"No findings\" or one or more disease classes:\n\n- Atelectasis\n- Consolidation\n- Infiltration\n- Pneumothorax\n- Edema\n- Emphysema\n- Fibrosis\n- Effusion\n- Pneumonia\n- Pleural_thickening\n- Cardiomegaly\n-  Nodule Mass\n-  Hernia\n"},{"metadata":{"_uuid":"3bb57dd4f3253b41cda1c48aaa201e04a6beafc0"},"cell_type":"markdown","source":"\n### Goal\n\nThe goal is to use a simple model to classify x-ray images in Keras, the notebook how to use the flow_from_dataframe to deal with messier datasets\n"},{"metadata":{"trusted":true,"_uuid":"0ef0657e6a818dbcc14d48bd3c885fac3d579fef"},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom glob import glob\nimport os\nimport matplotlib.gridspec as gridspec\nimport matplotlib.ticker as ticker\nsns.set_style('whitegrid')\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17f38ebecd78d5769a791b72177a0d6f59e07b5f"},"cell_type":"markdown","source":"<h1> 3. Exploratory Data Analysis </h1>"},{"metadata":{"_uuid":"a04857841c657f63f00d074019ff95fe9d3d7d8c"},"cell_type":"markdown","source":"<h2> 3.1 Data Loading </h2>"},{"metadata":{"trusted":true,"_uuid":"f1251d597b848bba2efdf6f321905df851d11f40"},"cell_type":"code","source":"# reading the data\ndata = pd.read_csv(\"../input/Data_Entry_2017.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a281974b4e2698b59653dd29983d3c1a8f94a56"},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"999e01cbc46a03e0db5058cd9f9fae2fdf16c19c"},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21e1f8b3a14f4e7ba06f92ba4c1fb8aaa9ccb4f1"},"cell_type":"markdown","source":"<h2> 3.2 Data cleaning </h2>"},{"metadata":{"trusted":true,"_uuid":"42ff65a04c51e2e7c9bd86a56ebff36e9dbb4df4"},"cell_type":"code","source":"#drop unused columns\ndata = data[['Image Index','Finding Labels','Follow-up #','Patient ID','Patient Age','Patient Gender']]\n\n# removing the rows which have patient_age >100\ntotal = len(data)\nprint('No. of rows before removing rows having age >100 : ',len(data))\ndata = data[data['Patient Age']<100]\nprint('No. of rows after removing rows having age >100 : ',len(data))\nprint('No. of datapoints having age > 100 : ',total-len(data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"738832fadca12c6217526c35c0ed21f841e80611"},"cell_type":"code","source":"# rows having no. of disease\ndata['Labels_Count'] = data['Finding Labels'].apply(lambda text: len(text.split('|')) if(text != 'No Finding') else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c87da841418c259d9ff95629854238b3453f01c"},"cell_type":"code","source":"label_counts = data['Finding Labels'].value_counts()[:15]\nfig, ax1 = plt.subplots(1,1,figsize = (12, 8))\nax1.bar(np.arange(len(label_counts))+0.5, label_counts)\nax1.set_xticks(np.arange(len(label_counts))+0.5)\n_ = ax1.set_xticklabels(label_counts.index, rotation = 90)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9be4c62385a62fcd4a3237654389d36870df2e2a"},"cell_type":"markdown","source":"<h2> 3.3 Data analysis </h2>"},{"metadata":{"trusted":true,"_uuid":"d5cc7f3e39b0c583ff631ea74c256de18489d5c6"},"cell_type":"markdown","source":"<h3> 3.3.1 Age distribution </h3>"},{"metadata":{"_uuid":"aa7da4cfe1f7df6687651fb22c540e1d1a77b112"},"cell_type":"markdown","source":"#### Observation - Both the gender have almost same distribution"},{"metadata":{"_uuid":"c79de3f3ef400aa57742564c1b748c6a8bcc86b9"},"cell_type":"markdown","source":"<h3> 3.3.2 Disease distribution by age and sex </h3>"},{"metadata":{"_uuid":"8edeac7a806d65bd362f7e9430aef7a3bced7d96"},"cell_type":"markdown","source":"<h3> 3.3.3 No. of each disease by patient gender </h3>"},{"metadata":{"_uuid":"03ace00114e431226ae086ae602b7ef9b565e8c1"},"cell_type":"markdown","source":"<h3> 3.3.4 Display patient number by Follow-up in details </h3>"},{"metadata":{"_uuid":"e05a08f3575491a1d2a4ae73f824e29882fb70ad"},"cell_type":"markdown","source":"<h3> 3.3.5 ratio between one and multiple disease </h3>"},{"metadata":{"trusted":true,"_uuid":"44675b1e556de7d5c35bf93d335d3605ac9558f4"},"cell_type":"markdown","source":"<h3> 3.3.6 Plot most important pathologies groups for each desease </h3>"},{"metadata":{"_uuid":"4899857bd75ea5b12305429d7611a182c3647f60"},"cell_type":"markdown","source":"<h1> 4. Creating data for model </h1>"},{"metadata":{"trusted":true,"_uuid":"600d7039b60f31992a67b6ca924f60d2bce187b1"},"cell_type":"code","source":"data = pd.read_csv('../input/Data_Entry_2017.csv')\ndata = data[data['Patient Age']<100] #removing datapoints which having age greater than 100\ndata_image_paths = {os.path.basename(x): x for x in \n                   glob(os.path.join('..', 'input', 'images*', '*', '*.png'))}\nprint('Scans found:', len(data_image_paths), ', Total Headers', data.shape[0])\ndata['path'] = data['Image Index'].map(data_image_paths.get)\ndata['Patient Age'] = data['Patient Age'].map(lambda x: int(x))\ndata.sample(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c51e6353264ab9ec25e65fb2608f0174d370afc4"},"cell_type":"code","source":"data['Finding Labels'] = data['Finding Labels'].map(lambda x: x.replace('No Finding', ''))\nfrom itertools import chain\nall_labels = np.unique(list(chain(*data['Finding Labels'].map(lambda x: x.split('|')).tolist())))\nall_labels = [x for x in all_labels if len(x)>0]\nprint('All Labels ({}): {}'.format(len(all_labels), all_labels))\nfor c_label in all_labels:\n    if len(c_label)>1: # leave out empty labels\n        data[c_label] = data['Finding Labels'].map(lambda finding: 1.0 if c_label in finding else 0)\ndata.sample(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"048e1c4bd4d0f05777660cdadad719ff6da4081d"},"cell_type":"code","source":"# keep at least 1000 cases\nMIN_CASES = 1000\nall_labels = [c_label for c_label in all_labels if data[c_label].sum()>MIN_CASES]\nprint('Clean Labels ({})'.format(len(all_labels)), \n      [(c_label,int(data[c_label].sum())) for c_label in all_labels])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f3404efcf52fae25b0cc4453a9bff8aefc52584"},"cell_type":"code","source":"# since the dataset is very unbiased, we can resample it to be a more reasonable collection\n# weight is 0.04 + number of findings\nsample_weights = data['Finding Labels'].map(lambda x: len(x.split('|')) if len(x)>0 else 0).values + 4e-2\nsample_weights /= sample_weights.sum()\ndata = data.sample(40000, weights=sample_weights)\n\nlabel_counts = data['Finding Labels'].value_counts()[:15]\nfig, ax1 = plt.subplots(1,1,figsize = (12, 8))\nax1.bar(np.arange(len(label_counts))+0.5, label_counts)\nax1.set_xticks(np.arange(len(label_counts))+0.5)\n_ = ax1.set_xticklabels(label_counts.index, rotation = 90)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58c0a0a826f66189e32291e3a94156cb262e8cbf"},"cell_type":"code","source":"# creating vector of diseases\ndata['disease_vec'] = data.apply(lambda x: [x[all_labels].values], 1).map(lambda x: x[0])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e4d735b73ab9142020bf8629d7121b6183586d2"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_df, valid_df = train_test_split(data, \n                                   test_size = 0.25, \n                                   random_state = 2018,\n                                   stratify = data['Finding Labels'].map(lambda x: x[:4]))\nprint('train', train_df.shape[0], 'validation', valid_df.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25840141a5f2ef1033e62caa23ce544435c3b5c8"},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\nIMG_SIZE = (128, 128)\ncore_idg = ImageDataGenerator(samplewise_center=True, \n                              samplewise_std_normalization=True, \n                              horizontal_flip = True, \n                              vertical_flip = False, \n                              height_shift_range= 0.05, \n                              width_shift_range=0.1, \n                              rotation_range=5, \n                              shear_range = 0.1,\n                              fill_mode = 'reflect',\n                              zoom_range=0.15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93bbc10d73ba2a2872ed1c4e77fd0fa96a3574fa"},"cell_type":"code","source":"def flow_from_dataframe(img_data_gen, in_df, path_col, y_col, **dflow_args):\n    base_dir = os.path.dirname(in_df[path_col].values[0])\n    print('## Ignore next message from keras, values are replaced anyways')\n    df_gen = img_data_gen.flow_from_directory(base_dir, \n                                     class_mode = 'sparse',\n                                    **dflow_args)\n    df_gen.filenames = in_df[path_col].values\n    df_gen.classes = np.stack(in_df[y_col].values)\n    df_gen.samples = in_df.shape[0]\n    df_gen.n = in_df.shape[0]\n    df_gen._set_index_array()\n    df_gen.directory = '' # since we have the full path\n    print('Reinserting dataframe: {} images'.format(in_df.shape[0]))\n    return df_gen\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40c777bc9b0e79ad909a95a1083ed176cc7cadbf"},"cell_type":"code","source":"train_gen = flow_from_dataframe(core_idg, train_df, \n                             path_col = 'path',\n                            y_col = 'disease_vec', \n                            target_size = IMG_SIZE,\n                             color_mode = 'rgb',\n                            batch_size = 32)\n\nvalid_gen = flow_from_dataframe(core_idg, valid_df, \n                             path_col = 'path',\n                            y_col = 'disease_vec', \n                            target_size = IMG_SIZE,\n                             color_mode = 'rgb',\n                            batch_size = 256) # we can use much larger batches for evaluation\n# used a fixed dataset for evaluating the algorithm\ntest_X, test_Y = next(flow_from_dataframe(core_idg, \n                               valid_df, \n                             path_col = 'path',\n                            y_col = 'disease_vec', \n                            target_size = IMG_SIZE,\n                             color_mode = 'rgb',\n                            batch_size = 1024)) # one big batch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61afcb0c604d696ffe1710d356c0021539a152ab"},"cell_type":"code","source":"t_x, t_y = next(train_gen)\nfig, m_axs = plt.subplots(4, 4, figsize = (16, 16))\nfor (c_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()):\n    c_ax.imshow(c_x[:,:,0], cmap = 'bone', vmin = -1.5, vmax = 1.5)\n    c_ax.set_title(', '.join([n_class for n_class, n_score in zip(all_labels, c_y) \n                             if n_score>0.5]))\n    c_ax.axis('off')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45266a56852910c74f75207f462c48c102b681a8"},"cell_type":"markdown","source":"### Create a simple model\n\nHere we make a simple model to train using MobileNet as a base and then adding a GAP layer (Flatten could also be added), dropout, and a fully-connected layer to calculate specific features\n"},{"metadata":{"trusted":true,"_uuid":"d6d3adccd4486a11391c402d7f74a1d25f896256"},"cell_type":"code","source":"from keras.applications.mobilenet import MobileNet\nfrom keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten\nfrom keras.models import Sequential\nmobilenet_model = MobileNet(input_shape =  t_x.shape[1:], \n                                 include_top = False, weights = None)\nmulti_disease_model = Sequential()\nmulti_disease_model.add(mobilenet_model)\nmulti_disease_model.add(GlobalAveragePooling2D())\nmulti_disease_model.add(Dropout(0.5))\nmulti_disease_model.add(Dense(512))\nmulti_disease_model.add(Dropout(0.5))\nmulti_disease_model.add(Dense(len(all_labels), activation = 'sigmoid'))\nmulti_disease_model.compile(optimizer = 'adam', loss = 'binary_crossentropy',\n                           metrics = ['binary_accuracy', 'mae'])\nmulti_disease_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87359fcbbbe4b286353f9d109e2688121fd128f2"},"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nweight_path=\"{}_weights.best.hdf5\".format('xray_class')\n\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n                             save_best_only=True, mode='min', save_weights_only = True)\n\nearly = EarlyStopping(monitor=\"val_loss\", \n                      mode=\"min\", \n                      patience=3)\ncallbacks_list = [checkpoint, early]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2f5619dc97fa6513d0ae46ed0c8f71aa549efa4"},"cell_type":"markdown","source":"### First Round\n\nHere we do a first round of training to get a few initial low hanging fruit results\n"},{"metadata":{"trusted":true,"_uuid":"4b0a7b1f2ddf61ec3c464d9c27f6e967bee1f94c"},"cell_type":"code","source":"multi_disease_model.fit_generator(train_gen, \n                                  steps_per_epoch=100,\n                                  validation_data = (test_X, test_Y), \n                                  epochs = 10, \n                                  callbacks = callbacks_list)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b33528474ffe4b66a2106af1e5ce58d352bf58d1"},"cell_type":"markdown","source":"### Check Output\n\nHere we see how many positive examples we have of each category\n"},{"metadata":{"trusted":true,"_uuid":"945aeb37ca2a953d0584452912d6dcf7ef99d518"},"cell_type":"code","source":"for c_label, s_count in zip(all_labels, 100*np.mean(test_Y,0)):\n    print('%s: %2.2f%%' % (c_label, s_count))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2cc96f3448cf2c02aa96caeed0c7733bd0adbbe4"},"cell_type":"code","source":"pred_Y = multi_disease_model.predict(test_X, batch_size = 32, verbose = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34755df123dd038fcd8fa2a6b0a5b99b26614bcd"},"cell_type":"markdown","source":"### ROC Curves\n\nWhile a very oversimplified metric, we can show the ROC curve for each metric\n"},{"metadata":{"trusted":true,"_uuid":"b6af6d7c2337ebb211fcaefffcc0d87d0e646bc4"},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nfig, c_ax = plt.subplots(1,1, figsize = (9, 9))\nfor (idx, c_label) in enumerate(all_labels):\n    fpr, tpr, thresholds = roc_curve(test_Y[:,idx].astype(int), pred_Y[:,idx])\n    c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))\nc_ax.legend()\nc_ax.set_xlabel('False Positive Rate')\nc_ax.set_ylabel('True Positive Rate')\nfig.savefig('barely_trained_net.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install lime eli5 scikit-image xgboost --upgrade\n#!pip install scikit-learn==0.20\n#!pip install shap==0.31","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import shap\nimport numpy as np\n\n# select a set of background examples to take an expectation over\nfrom scipy import misc\nimage_path=train_df.iloc[5,12]\nbackground = misc.imread(image_path)\n\n\n\n\n# explain predictions of the model on three images\ne = shap.DeepExplainer(multi_disease_model, t_x[0:5])\n# ...or pass tensors directly\n#e = shap.DeepExplainer((multi_disease_model.layers[0].input, model.layers[-1].output), test_X)\nshap_values = e.shap_values(t_x[0:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.image_plot(shap_values, -t_x[1:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"shap_values[0][0,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.initjs()\nshap.force_plot(e.expected_value[1], shap_values[1][0,0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_to_1channel(images):\n    return images\n\ndef new_predict_fn(images):\n    image = convert_to_1channel(images)\n    return multi_disease_model.predict(images)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications import inception_v3 as inc_net\nfrom keras.preprocessing import image\nfrom skimage.util import img_as_float\nfrom sklearn.preprocessing import Normalizer\nfoo=t_x[0, :,:,0]\n#foo = np.expand_dims(foo, axis=0)\ntransformer = Normalizer().fit(foo)\nfoo2=transformer.transform(foo)\nfoo3=img_as_float(foo2)\n#foo3 = np.expand_dims(foo3, axis=-1)\n#foo3 = np.expand_dims(foo3, axis=0)\n#foo3 = np.stack((foo3,)*3, axis=-1)\n#foo3.reshape(128,128,3)\n\nprint(foo3.shape)\nprint('shape')\nprint(len(foo))\n\n\n#img = image.load_img(image_path, target_size=(128, 128))\n#x = image.img_to_array(img)\n#x = np.expand_dims(x, axis=0)\n#x = inc_net.preprocess_input(x)\n\nimport lime\nfrom lime import lime_image\nexplainer = lime_image.LimeImageExplainer()\n#foo[foo < -1.0] = 0.1\n#foo[foo > 1.0] = 0.1\n#print(foo.shape)\nexplanation = explainer.explain_instance(foo2,new_predict_fn, top_labels=5, hide_color=0,batch_size='None')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"explanation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\npics= [test_X[x,:,:,0] for x in range(100)]\nres=[]\n                  \nfor i in pics:\n    transformer= Normalizer().fit(i)\n    foo2=transformer.transform(i)\n    foo3=img_as_float(foo2)\n    res.append(foo3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from lime.wrappers.scikit_image import SegmentationAlgorithm\nrandom_seed = 1\nsegmenter = SegmentationAlgorithm('quickshift', kernel_size=4, max_dist=200,\n                                  ratio=0.2, random_seed=random_seed)\ndef bulk_exp(explainer, images):\n    return [explainer.explain_instance(image, new_predict_fn, top_labels=5,\n                                        hide_color=0, num_samples=1000\n                                        )   for image in images]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_identity(img1, img2, verbose=True):\n    dis = np.array([np.array_equal(img1[i],img2[i]) for i in range(len(img1))])\n    total = dis.shape[0]\n    true = np.sum(dis)\n    score = (total-true)/total\n    if verbose:\n        print('true: ',true, 'wrong: ', total-true, 'total: ', total)\n    return score*100, true, total\n\ndef calc_separability(exp):\n    wrong = 0\n    for i in range(exp.shape[0]):\n        for j in range(exp.shape[0]):\n            if i == j:\n                continue\n            eq = np.array_equal(exp[i],exp[j])\n            if eq:\n                wrong = wrong + 1\n    total = exp.shape[0]\n    score = 100*abs(wrong)/(total**2-total)\n    print('true: ', total**2-total-wrong, 'wrong: ', wrong, 'total: ', total**2-total)\n    return wrong,total,total**2-total,score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn\nimport sklearn.cluster\nfrom sklearn.manifold import TSNE\n\n\ndef calc_stability2(exp, labels):\n    total = labels.shape[0]\n    label_values = np.unique(labels)\n    print(label_values)\n    n_clusters = label_values.shape[0]\n    #init = np.array([[np.average(exp[np.where(labels == i)], axis = 0)] for i in label_values]).squeeze()\n    ct = sklearn.cluster.KMeans(n_clusters = n_clusters, n_jobs=5, random_state=1)\n    #ct=TSNE(n_components=2)\n    ct.fit(exp)\n    print(ct.labels_)\n    error = np.sum(np.abs(labels-ct.labels_))\n    if error/total > 0.5:\n        error = total-error\n    return error, total","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(calc_stability2(picList,ansNum))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pics_y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"picList=[]\nfor i in range(100):\n    picList.append(test_X[i,:,:,:].flatten())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(t_x)\nX_train = np.array(res).reshape(len(t_x),-1)\nX_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[test_X[0:6:,:,1]].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nm=shap_values[0]\nm.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.array([2,3,4]).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pics_y= np.array([test_Y[x] for x in range(5)])\n\nansList=[]\nansNum=[]\nj=0\nfor i in range(100):\n    result=np.where(test_Y[i]==1.0)\n    #result=np.where(test_Y[i]=1.0,test_Y[i])\n    if result[0].tolist() not in ansList:\n        print(\"here\")\n        ansList.append(result[0].tolist())\n        ansNum.append(j)\n        j=j+1\n    else:\n        index=ansList.index(result[0].tolist())\n        ansNum.append(index)\n        \n    \nansNum=np.array(ansNum)\nprint(ansList)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ansNum","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#np.where(np.isclose(test_Y[0], 1.0))\nansList2=[np.where(test_Y[65]==1.0)[0].tolist()]\nif np.where(test_Y[66]==1.0)[0].tolist() not in ansList2:\n    ansList2.append(np.where(test_Y[66]==1.0)[0].tolist())\n    print('Noo')\nelse:\n    print('Yes')\n    \nprint(ansList2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time exps1 = bulk_exp(explainer, res)\n%time exps2 = bulk_exp(explainer, res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(test_X[0,:,:,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"no_superpixels = 5\ndef get_imgs_from_exps(exps):\n    return np.array([exp.get_image_and_mask(exp.top_labels[0], positive_only=True,\n                                            num_features=no_superpixels, hide_rest=True)[0] for exp in exps])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time imgs1 = get_imgs_from_exps(exps1)\n%time imgs2 = get_imgs_from_exps(exps2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(imgs1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calc_identity(imgs1,imgs2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calc_separability(imgs1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SHAP testing\n#%time imgs3 = get_imgs_from_exps(exps1[0])\n#%time imgs4 = get_imgs_from_exps(exps2[0])\nshap_values3 = e.shap_values(t_x[0:100])\nshap_values2 = e.shap_values(t_x[0:100])\n\ncalc_identity(shap_values3,shap_values2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calc_separability(shap_values[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from skimage.segmentation import mark_boundaries\ntemp, mask = explanation.get_image_and_mask(explanation.top_labels[1], positive_only=True, num_features=5, hide_rest=True)\nplt.imshow(mark_boundaries(temp / 2 + 0.5, mask))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(foo3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, num_features=10, hide_rest=False)\nplt.imshow(mark_boundaries(temp / 2 + 0.5, mask))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install anchor-exp\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import anchor\nexplainer2= anchor.AnchorImage()\nexplanation = explainer2.explain_instance(foo3,new_predict_fn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_X[0, :,:,0].shape)\nprint(test_X[0, :,:,0].shape)\nplt.imshow(test_X[0, :,:,0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb544a45b540d20e47889c73d2cd21bd725c9d00"},"cell_type":"markdown","source":"### Continued Training\n\nNow we do a much longer training process to see how the results improve\n"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"a1d7e0cbe0e7b4ab46ba198190e197e54cc7aa0f"},"cell_type":"code","source":"multi_disease_model.fit_generator(train_gen, \n                                  steps_per_epoch = 100,\n                                  validation_data =  (test_X, test_Y), \n                                  epochs = 5, \n                                  callbacks = callbacks_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7236bf34acfac38ff73f9789bb16a8de8a89b351"},"cell_type":"code","source":"# load the best weights\nmulti_disease_model.load_weights(weight_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b63fc693293bc4710dde94ebcb9c0bb0b273bf05"},"cell_type":"code","source":"pred_Y = multi_disease_model.predict(test_X, batch_size = 32, verbose = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77ab3a8b11d69588795660fcd6c214bd821d1f06"},"cell_type":"code","source":"# look at how often the algorithm predicts certain diagnoses \nfor c_label, p_count, t_count in zip(all_labels, \n                                     100*np.mean(pred_Y,0), \n                                     100*np.mean(test_Y,0)):\n    print('%s: Dx: %2.2f%%, PDx: %2.2f%%' % (c_label, t_count, p_count))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64f3b1d45725b1897200b07c207ecaac67027573"},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nfig, c_ax = plt.subplots(1,1, figsize = (9, 9))\nfor (idx, c_label) in enumerate(all_labels):\n    fpr, tpr, thresholds = roc_curve(test_Y[:,idx].astype(int), pred_Y[:,idx])\n    c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))\nc_ax.legend()\nc_ax.set_xlabel('False Positive Rate')\nc_ax.set_ylabel('True Positive Rate')\nfig.savefig('trained_net.png')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65b62a548793d9e537151132fbaec828f16eb9b5"},"cell_type":"markdown","source":"### Show a few images and associated predictions"},{"metadata":{"trusted":true,"_uuid":"837e37ab716e8db2cbccb477e103b7b864915d38"},"cell_type":"code","source":"sickest_idx = np.argsort(np.sum(test_Y, 1)<1)\nfig, m_axs = plt.subplots(4, 2, figsize = (16, 32))\nfor (idx, c_ax) in zip(sickest_idx, m_axs.flatten()):\n    c_ax.imshow(test_X[idx, :,:,0], cmap = 'bone')\n    stat_str = [n_class[:6] for n_class, n_score in zip(all_labels, \n                                                                  test_Y[idx]) \n                             if n_score>0.5]\n    pred_str = ['%s:%2.0f%%' % (n_class[:4], p_score*100)  for n_class, n_score, p_score in zip(all_labels, \n                                                                  test_Y[idx], pred_Y[idx]) \n                             if (n_score>0.5) or (p_score>0.5)]\n    c_ax.set_title('Dx: '+', '.join(stat_str)+'\\nPDx: '+', '.join(pred_str))\n    c_ax.axis('off')\nfig.savefig('trained_img_predictions.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6046eb7ec6d84025a87248da844f743e342d2a7f"},"cell_type":"code","source":"import shap\nimport numpy as np\n\n# select a set of background examples to take an expectation over\nfrom scipy import misc\nimage_path=train_df.iloc[5,12]\nbackground = misc.imread(image_path)\n\n\n\n\n# explain predictions of the model on three images\ne = shap.DeepExplainer(multi_disease_model, t_x[0].reshape(1, 128, 128, 1))\n# ...or pass tensors directly\ne = shap.DeepExplainer((multi_disease_model.layers[0].input, model.layers[-1].output), background)\nshap_values = e.shap_values(x_test[1:5])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}